{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc1ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time, math\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64534a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a47243",
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_path =  \"scan20_splits.npz\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad31eaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved keys: ['ksp', 'sens', 'theta', 'lam', 'val', 'omega', 'split_seed']\n"
     ]
    }
   ],
   "source": [
    "data = np.load(npz_path, allow_pickle=False)\n",
    "print(\"Saved keys:\", list(data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8455c644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_torch(x, dtype=None, dev=device):\n",
    "    t = torch.from_numpy(x)\n",
    "    if dtype is not None:\n",
    "        t = t.to(dtype)\n",
    "    return t.to(dev, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34fdc9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ksp    = to_torch(data[\"ksp\"],   dtype=torch.complex64)   # (P,C,X,Y,Z)\n",
    "sens   = to_torch(data[\"sens\"],  dtype=torch.complex64)   # (C,X,Y,Z)\n",
    "theta  = to_torch(data[\"theta\"], dtype=torch.bool)        # (P,Y,Z)\n",
    "lam  = to_torch(data[\"lam\"],   dtype=torch.bool)        # (P,Y,Z)\n",
    "val  = to_torch(data[\"val\"], dtype=torch.bool)        # (P,Y,Z)\n",
    "omega = to_torch(data[\"omega\"], dtype=torch.bool)        # (P,Y,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fdf40fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ksp:   (20, 34, 128, 120, 80)  torch.complex64\n",
      "sens:  (34, 128, 120, 80) torch.complex64\n",
      "theta: (20, 120, 80) torch.bool\n",
      "lam:   (20, 120, 80) torch.bool\n",
      "val: (20, 120, 80) torch.bool\n",
      "omega: (20, 120, 80) torch.bool\n"
     ]
    }
   ],
   "source": [
    "P, C, X, Y, Z = ksp.shape\n",
    "print(f\"ksp:   {tuple(ksp.shape)}  {ksp.dtype}\")\n",
    "print(f\"sens:  {tuple(sens.shape)} {sens.dtype}\")\n",
    "print(f\"theta: {tuple(theta.shape)} {theta.dtype}\")\n",
    "print(f\"lam:   {tuple(lam.shape)} {lam.dtype}\")\n",
    "print(f\"val: {tuple(val.shape)} {val.dtype}\")\n",
    "print(f\"omega: {tuple(omega.shape)} {omega.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b9fb372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_broadcast_masks(theta, lam_mask, gamma, omega=None):\n",
    "    \"\"\"\n",
    "    Inputs: (P,Y,Z) bool\n",
    "    Returns: dict of (P,1,1,Y,Z) bool (contiguous, on same device)\n",
    "    Also includes complements *_c for convenience.\n",
    "    \"\"\"\n",
    "    def b(m): \n",
    "        return m[:, None, None, ...].contiguous()\n",
    "\n",
    "    masks = {\n",
    "        \"theta\": b(theta),\n",
    "        \"lam\":   b(lam_mask),\n",
    "        \"gamma\": b(gamma),\n",
    "    }\n",
    "    if omega is not None:\n",
    "        masks[\"omega\"] = b(omega)\n",
    "\n",
    "    # assertion\n",
    "    P, _, _, Y, Z = masks[\"theta\"].shape\n",
    "    assert masks[\"lam\"].shape   == (P,1,1,Y,Z)\n",
    "    assert masks[\"gamma\"].shape == (P,1,1,Y,Z)\n",
    "    if \"omega\" in masks:\n",
    "        assert masks[\"omega\"].shape == (P,1,1,Y,Z)\n",
    "\n",
    "    return masks\n",
    "\n",
    "bmasks = precompute_broadcast_masks(theta, lam, val, omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "743affe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 1, 120, 80])\n",
      "torch.Size([20, 1, 1, 120, 80])\n",
      "torch.Size([20, 1, 1, 120, 80])\n"
     ]
    }
   ],
   "source": [
    "bmasks.keys()\n",
    "print(bmasks['theta'].shape)\n",
    "print(bmasks['lam'].shape)\n",
    "print(bmasks['gamma'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02819ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNAct(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=3):\n",
    "        super().__init__()\n",
    "        p = k//2\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, k, padding=p)\n",
    "        self.gn   = nn.GroupNorm(num_groups=8, num_channels=out_ch)\n",
    "        self.act  = nn.LeakyReLU(0.1, inplace=True)\n",
    "    def forward(self, x):\n",
    "        return self.act(self.gn(self.conv(x)))\n",
    "\n",
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.c1 = ConvBNAct(in_ch,  out_ch)\n",
    "        self.c2 = ConvBNAct(out_ch, out_ch)\n",
    "    def forward(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.c2(x)\n",
    "        return x\n",
    "\n",
    "class Pseudo3DUNet2p5D(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:  (B, 2*k, H, W) — k neighboring slices, real/imag stacked as channels\n",
    "    Output: (B, 2,   H, W) — residual for center slice (real, imag)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=10, base=32, out_ch=2, residual_scale=0.1):\n",
    "        super().__init__()\n",
    "        self.enc1 = UNetBlock(in_ch,   base)\n",
    "        self.enc2 = UNetBlock(base,    base*2)\n",
    "        self.enc3 = UNetBlock(base*2,  base*4)\n",
    "\n",
    "        self.down1 = nn.Conv2d(base,   base,   3, stride=2, padding=1)\n",
    "        self.down2 = nn.Conv2d(base*2, base*2, 3, stride=2, padding=1)\n",
    "\n",
    "        self.dec2 = UNetBlock(base*4 + base*2, base*2)\n",
    "        self.dec1 = UNetBlock(base*2 + base,   base)\n",
    "\n",
    "        self.out  = nn.Conv2d(base, out_ch, 1)\n",
    "        self.res_scale = residual_scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)                  # (B, B, H, W)\n",
    "        x  = self.down1(e1)                # (B, B, H/2, W/2)\n",
    "        e2 = self.enc2(x)                  # (B, 2B, H/2, W/2)\n",
    "        x  = self.down2(e2)                # (B, 2B, H/4, W/4)\n",
    "        e3 = self.enc3(x)                  # (B, 4B, H/4, W/4)\n",
    "\n",
    "        # Decoder\n",
    "        x  = F.interpolate(e3, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        x  = torch.cat([x, e2], dim=1)\n",
    "        x  = self.dec2(x)\n",
    "\n",
    "        x  = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        x  = torch.cat([x, e1], dim=1)\n",
    "        x  = self.dec1(x)\n",
    "\n",
    "        out = self.out(x)\n",
    "        return out * self.res_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acce07f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def A(x: torch.Tensor,\n",
    "      mask: torch.Tensor,\n",
    "      sens: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Forward model: image → undersampled k-space.\n",
    "    x    : [X, Y, Z] complex64\n",
    "    mask : [1, 1, Y, Z]\n",
    "    sens : [C, X, Y, Z]\n",
    "    returns y : [C, X, Y, Z] complex64\n",
    "    \"\"\"\n",
    "    # coil images: [C, X, Y, Z]\n",
    "    x_coils = sens * x.unsqueeze(0)\n",
    "    # full 3D FFT on (X,Y,Z)\n",
    "    y_full = torch.fft.fftn(x_coils, dim=(1,2,3), norm=\"ortho\")\n",
    "    # apply sampling mask (broadcasts over coil & X)\n",
    "    return y_full * mask\n",
    "\n",
    "\n",
    "def AH(y: torch.Tensor,\n",
    "       mask: torch.Tensor,\n",
    "       sens: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Adjoint model: undersampled k-space → image.\n",
    "    y    : [C, X, Y, Z] complex64\n",
    "    mask : [1, 1, Y, Z]\n",
    "    sens : [C, X, Y, Z]\n",
    "    returns x : [X, Y, Z] complex64\n",
    "    \"\"\"\n",
    "    # zero out unmeasured lines\n",
    "    y_masked = y * mask\n",
    "    # inverse 3D FFT\n",
    "    x_coils = torch.fft.ifftn(y_masked, dim=(1,2,3), norm=\"ortho\")\n",
    "    # coil-combine: sum conj(sens) * coil images\n",
    "    return torch.sum(torch.conj(sens) * x_coils, dim=0)\n",
    "\n",
    "\n",
    "def normal_op(x: torch.Tensor,\n",
    "              mask: torch.Tensor,\n",
    "              sens: torch.Tensor,\n",
    "              lam: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    (A^H A + λ I) x\n",
    "    x    : [X, Y, Z] complex64\n",
    "    mask : [1, 1, Y, Z]\n",
    "    sens : [C, X, Y, Z]\n",
    "    lam  : float\n",
    "    \"\"\"\n",
    "    return AH(A(x, mask, sens), mask, sens) + lam * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3a4be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cg_sense(\n",
    "    rhs, mask, sens, lam,\n",
    "    iters=8, tol=1e-3, x0=None,\n",
    "    A=A, AH=AH\n",
    "):\n",
    "    \"\"\"\n",
    "    CG-SENSE with optional warm-start x0 and early-stop on relative residual.\n",
    "    Returns (x, info_dict).\n",
    "    \"\"\"\n",
    "    def normal_op(x):\n",
    "        return AH(A(x, mask, sens), mask, sens) + lam * x\n",
    "\n",
    "    x = rhs.clone() if x0 is None else x0.clone()\n",
    "    r = rhs - normal_op(x)\n",
    "    p = r.clone()\n",
    "    rsold = torch.vdot(r.flatten(), r.flatten()).real\n",
    "\n",
    "    relres_traj = []\n",
    "    for i in range(1, iters + 1):\n",
    "        Ap = normal_op(p)\n",
    "        alpha = rsold / torch.vdot(p.flatten(), Ap.flatten()).real\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * Ap\n",
    "        rsnew = torch.vdot(r.flatten(), r.flatten()).real\n",
    "        relres = (rsnew.sqrt() / (rhs.norm() + 1e-12)).item()\n",
    "        relres_traj.append(relres)\n",
    "        if relres < tol:\n",
    "            break\n",
    "        beta = rsnew / rsold\n",
    "        p = r + beta * p\n",
    "        rsold = rsnew\n",
    "\n",
    "    return x, {\"iters\": i, \"relres\": relres_traj}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d99a1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_prior_25d(prior25d: nn.Module, x3d: torch.Tensor, k: int = 5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x3d: complex tensor (X, Y, Z)\n",
    "    returns: complex residual (X, Y, Z)\n",
    "    \"\"\"\n",
    "    assert x3d.is_complex()\n",
    "    X, Y, Z = x3d.shape\n",
    "    r = k // 2\n",
    "\n",
    "    # pad along Z with replication\n",
    "    x5  = x3d.unsqueeze(0).unsqueeze(0)\n",
    "    x5p = F.pad(x5, pad=(r, r, 0, 0, 0, 0), mode='replicate')  # (X, Y, Z+2r)\n",
    "\n",
    "    # build (Z, 2*k, X, Y) batch for the UNet\n",
    "    slabs = []\n",
    "    for z in range(Z):\n",
    "        slab = x5p[..., z:z+k].squeeze(0).squeeze(0)                 # (X, Y, k) complex\n",
    "        real = slab.real.permute(2, 0, 1)        # (k, X, Y)\n",
    "        imag = slab.imag.permute(2, 0, 1)        # (k, X, Y)\n",
    "        inp  = torch.cat([real, imag], dim=0)    # (2k, X, Y)\n",
    "        slabs.append(inp.contiguous())\n",
    "    inp_batch = torch.stack(slabs, dim=0)        # (Z, 2k, X, Y)\n",
    "\n",
    "    # run prior in batch over slices\n",
    "    out = prior25d(inp_batch)                    # (Z, 2, X, Y)\n",
    "\n",
    "    # back to complex (X, Y, Z)\n",
    "    out_c = torch.complex(out[:,0], out[:,1]).permute(1,2,0).contiguous()\n",
    "    return out_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fff01b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kspace_l1_l2_loss(y_pred, y_true, alpha=0.5, eps=1e-6):\n",
    "    diff = y_pred - y_true\n",
    "    mag  = torch.abs(diff)\n",
    "    l1   = mag.mean()\n",
    "    l2   = (mag**2).mean()\n",
    "    return alpha*l1 + (1-alpha)*l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e1700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_cg(rhs, mask, sens, lam, iters, tol, x0=None):\n",
    "    out = cg_sense(rhs, mask, sens, lam, iters=iters, tol=tol, x0=x0)\n",
    "    return out[0] if isinstance(out, tuple) else out\n",
    "\n",
    "@torch.no_grad()\n",
    "def _tensor_float(x):\n",
    "    return float(x.detach().cpu().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d50e31cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lambda_schedule(lam_start=3e-2, lam_end=5e-3, T=30):\n",
    "    \"\"\"\n",
    "    Cosine decay from lam_start → lam_end over T epochs.\n",
    "    After T epochs it stays at lam_end.\n",
    "    \"\"\"\n",
    "    def lam_at_epoch(ep):\n",
    "        t = min(max(ep-1, 0), T-1) / max(T-1, 1)\n",
    "        w = 0.5*(1 + math.cos(math.pi * t))  # 1→0\n",
    "        return lam_end + (lam_start - lam_end) * w\n",
    "    return lam_at_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8540acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckpt(path, epoch, val_loss, model, opt):\n",
    "    torch.save(\n",
    "        {\"epoch\": epoch, \"val_loss\": val_loss,\n",
    "         \"model_state\": model.state_dict(),\n",
    "         \"optim_state\": opt.state_dict()},\n",
    "        path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62233ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    prior, opt,\n",
    "    ksp, sens, bmasks,\n",
    "    A, AH, solve_cg, apply_prior_2d, kspace_l1_l2_loss,\n",
    "    lam_param,                  # learnable λ parameter (nn.Parameter)\n",
    "    alpha=0.5,\n",
    "    unroll=5, cg_iters=8, cg_tol=1e-3,\n",
    "    k_neighbors=3, device=\"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Changes:\n",
    "      - ZF anchor: x_anchor = AH(y_theta)\n",
    "      - Learnable λ via softplus\n",
    "      - γ removed; residual scaled by λ (per request)\n",
    "    \"\"\"\n",
    "    prior.train()\n",
    "    P = ksp.shape[0]\n",
    "    phase_order = torch.randperm(P, device=device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_gn   = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    lam_eps = 1e-8  # keep λ strictly positive\n",
    "\n",
    "    for p in phase_order.tolist():\n",
    "        mask_t = bmasks[\"theta\"][p]   # (1,1,Y,Z)\n",
    "        mask_l = bmasks[\"lam\"][p]     # (1,1,Y,Z)\n",
    "\n",
    "        # Θ measurements\n",
    "        y_t   = ksp[p] * mask_t\n",
    "\n",
    "        # ZF anchor / adjoint\n",
    "        x_anchor = AH(y_t, mask_t, sens)\n",
    "\n",
    "        # learnable λ (shared for DC and residual scale)\n",
    "        lam = F.softplus(lam_param) + lam_eps\n",
    "\n",
    "        # unroll (CG in-graph)\n",
    "        x = x_anchor.clone()\n",
    "        for _ in range(unroll):\n",
    "            r     = apply_prior_2d(prior, x)\n",
    "            rhs   = x_anchor + lam * r                \n",
    "            x     = solve_cg(rhs, mask_t, sens, lam, cg_iters, cg_tol, x0=x)\n",
    "\n",
    "        # Λ loss\n",
    "        y_l   = ksp[p] * mask_l\n",
    "        y_hat = A(x, mask_l, sens)\n",
    "        loss  = kspace_l1_l2_loss(y_hat, y_l, alpha)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(prior.parameters(), 5e-5)\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += float(loss.item())\n",
    "\n",
    "        # simple average grad-norm over prior params for logging\n",
    "        try:\n",
    "            gsum, cnt = 0.0, 0\n",
    "            for q in prior.parameters():\n",
    "                if q.grad is not None:\n",
    "                    gsum += q.grad.detach().norm().item()\n",
    "                    cnt  += 1\n",
    "            if cnt > 0:\n",
    "                running_gn += gsum / cnt\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "    return {\n",
    "        \"train_L\": running_loss / max(steps, 1),\n",
    "        \"grad\":    running_gn   / max(steps, 1),\n",
    "        \"lam\":     _tensor_float(lam),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e2212",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate_all_phases(\n",
    "    prior,\n",
    "    ksp, sens, bmasks,\n",
    "    A, AH, solve_cg, apply_prior_25d, kspace_l1_l2_loss,\n",
    "    lam_param,                 # learnable λ (nn.Parameter)\n",
    "    alpha=0.5,\n",
    "    unroll=5, cg_iters=8, cg_tol=1e-3,\n",
    "    k_neighbors=3, device=\"cuda\"\n",
    "):\n",
    "    prior.eval()\n",
    "\n",
    "    P = ksp.shape[0]\n",
    "    val_losses = []\n",
    "    mΩ_list, eΩc_list = [], []\n",
    "\n",
    "    lam_eps = 1e-8\n",
    "    lam = F.softplus(lam_param) + lam_eps  # shared scale\n",
    "\n",
    "    ones_mask = None  # cache a full-ones mask for A(x, ·, sens)\n",
    "\n",
    "    for p in range(P):\n",
    "        mask_t = bmasks[\"theta\"][p]    # (1,1,Y,Z)\n",
    "        mask_l = bmasks[\"lam\"][p]      # (1,1,Y,Z)\n",
    "        mask_Ω = bmasks[\"omega\"][p]    # (1,1,Y,Z)\n",
    "\n",
    "        # Θ measurements and ZF anchor\n",
    "        y_t     = ksp[p] * mask_t\n",
    "        x_anchor = AH(y_t, mask_t, sens)\n",
    "\n",
    "        # unroll with λ-scaled residual (γ removed)\n",
    "        x = x_anchor.clone()\n",
    "        for _ in range(unroll):\n",
    "            r     = apply_prior_25d(prior, x)\n",
    "            rhs   = x_anchor + lam * r\n",
    "            x     = solve_cg(rhs, mask_t, sens, lam, cg_iters, cg_tol, x0=x)\n",
    "\n",
    "        # Λ-loss on Λ mask\n",
    "        y_l   = ksp[p] * mask_l\n",
    "        y_hat = A(x, mask_l, sens)\n",
    "        loss  = kspace_l1_l2_loss(y_hat, y_l, alpha)\n",
    "        val_losses.append(float(loss.item()))\n",
    "\n",
    "        # --- minimal Ω metrics (broadcast-safe) ---\n",
    "        if ones_mask is None:\n",
    "            ones_mask = torch.ones_like(mask_t, dtype=mask_t.dtype, device=mask_t.device)\n",
    "\n",
    "        y_full  = A(x, ones_mask, sens)                # (C,X,Y,Z)\n",
    "        diff2   = (y_full - ksp[p]).abs().pow(2)\n",
    "\n",
    "        mΩ      = mask_Ω.to(diff2.dtype)               # (1,1,Y,Z), broadcasts over (C,X)\n",
    "        mΩc     = (1.0 - mΩ)\n",
    "\n",
    "        C, X = diff2.shape[0], diff2.shape[1]\n",
    "        denom_Ω  = (mΩ.sum()  * C * X).clamp_min(1.0)\n",
    "        denom_Ωc = (mΩc.sum() * C * X).clamp_min(1.0)\n",
    "\n",
    "        mse_acq   = (diff2 * mΩ ).sum() / denom_Ω\n",
    "        energy_un = (y_full.abs().pow(2) * mΩc).sum() / denom_Ωc\n",
    "\n",
    "        mΩ_list.append(float(mse_acq.item()))\n",
    "        eΩc_list.append(float(energy_un.item()))\n",
    "\n",
    "    val_loss = sum(val_losses) / max(len(val_losses), 1)\n",
    "    mΩ_mean  = sum(mΩ_list)   / max(len(mΩ_list), 1)\n",
    "    eΩc_mean = sum(eΩc_list)  / max(len(eΩc_list), 1)\n",
    "\n",
    "    return val_loss, mΩ_mean, eΩc_mean, float(lam.detach().cpu().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fde953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- config ---\n",
    "epochs        = 100\n",
    "unroll_tr     = 5\n",
    "unroll_val    = 5\n",
    "cg_tr_iters   = 8\n",
    "cg_val_iters  = 25\n",
    "cg_tol_tr     = 1e-3\n",
    "cg_tol_val    = 1e-6\n",
    "alpha         = 0.5\n",
    "k_neighbors   = None\n",
    "\n",
    "prior = Pseudo3DUNet2p5D(in_ch=2*k_neighbors).to(device)\n",
    "\n",
    "# --- learnable λ: param in softplus space so λ=softplus(param) > 0 ---\n",
    "def inv_softplus(y):\n",
    "    # inverse of softplus for positive y\n",
    "    return torch.log(torch.expm1(torch.as_tensor(y, device=device)))\n",
    "\n",
    "lam_init = 3e-2\n",
    "lam_param = torch.nn.Parameter(inv_softplus(lam_init))  # so softplus(lam_param) ≈ 3e-2\n",
    "\n",
    "# --- optimizer + schedulers (two param groups: prior + λ) ---\n",
    "start_lr   = 5e-4\n",
    "lam_lr     = 1e-4   # usually smaller lr for λ is stabler\n",
    "opt = torch.optim.Adam([\n",
    "    {\"params\": prior.parameters(), \"lr\": start_lr, \"weight_decay\": 0.0},\n",
    "    {\"params\": [lam_param],        \"lr\": lam_lr,   \"weight_decay\": 0.0},\n",
    "])\n",
    "\n",
    "warmup_epochs = 3\n",
    "total_epochs  = epochs\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "    opt,\n",
    "    schedulers=[\n",
    "        torch.optim.lr_scheduler.LinearLR(opt, start_factor=0.2, total_iters=warmup_epochs),\n",
    "        torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=total_epochs - warmup_epochs, eta_min=5e-6),\n",
    "    ],\n",
    "    milestones=[warmup_epochs],\n",
    ")\n",
    "\n",
    "best_val = float('inf')\n",
    "\n",
    "for ep in range(epochs):\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_stats = train_one_epoch(\n",
    "        prior, opt,\n",
    "        ksp, sens, bmasks,\n",
    "        A, AH, solve_cg, apply_prior_25d, kspace_l1_l2_loss,\n",
    "        lam_param=lam_param, alpha=alpha,\n",
    "        unroll=unroll_tr, cg_iters=cg_tr_iters, cg_tol=cg_tol_tr,\n",
    "        k_neighbors=k_neighbors, device=device\n",
    "    )\n",
    "\n",
    "    val_loss, val_mΩ, val_eΩc, lam_scalar = validate_all_phases(\n",
    "        prior, ksp, sens, bmasks,\n",
    "        A, AH, solve_cg, apply_prior_25d, kspace_l1_l2_loss,\n",
    "        lam_param=lam_param, alpha=alpha,\n",
    "        unroll=unroll_val, cg_iters=cg_val_iters, cg_tol=cg_tol_val,\n",
    "        k_neighbors=k_neighbors, device=device\n",
    "    )\n",
    "\n",
    "    scheduler.step()\n",
    "    save_ckpt(\"latest_prior.pt\", ep+1, val_loss, prior, opt)  # (optional) extend to also save lam_param\n",
    "\n",
    "    if val_loss < best_val - 1e-9:\n",
    "        best_val = val_loss\n",
    "        save_ckpt(\"best_prior.pt\", ep+1, val_loss, prior, opt)  # (optional) extend to also save lam_param\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    print(\n",
    "        f\"[Epoch {ep:3d}] train_Λ={train_stats['train_L']:.3e}  ‖∇‖={train_stats['grad']:.3e}  \"\n",
    "        f\"val={val_loss:.3e}  (Ω_MSE={val_mΩ:.3e}, Ωc_E={val_eΩc:.3e})  \"\n",
    "        f\"λ={lam_scalar:.2e}  LR={opt.param_groups[0]['lr']:.1e}  time={dt:.1f}s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cf9803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: P=20, C=24, X=160, Y=128, Z=72\n",
      "phase 01 | γ=2.00e-02  Ω_MSE=5.003e-09  Ωc_E=1.048e-07\n",
      "\n",
      "Saved SoS volumes + CSV to: inference_all_phases\n",
      "Elapsed: 591.9s\n"
     ]
    }
   ],
   "source": [
    "# --- lean all-phases inference with per-phase gamma sweep ---\n",
    "import os, csv, time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ==== config ====\n",
    "npz_path     = \"scan20_splits.npz\"   # your saved data\n",
    "out_dir      = \"inference_all_phases\" # outputs here\n",
    "device       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lam_fixed    = 5e-2\n",
    "gamma_grid   = [2e-2, 3e-2, 5e-2]     # per-phase sweep\n",
    "unroll       = 5\n",
    "cg_iters     = 60\n",
    "cg_tol       = 1e-8\n",
    "k_neighbors  = 3\n",
    "\n",
    "energy_cap   = 1.10e-07               # Ωᶜ energy cap (mean power on unacquired)\n",
    "fallback_eta = 1.0                    # weight if nothing meets the cap\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# ==== helpers ====\n",
    "def to_torch(x):\n",
    "    return torch.from_numpy(x).to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def sos_from_image_and_sens(x_img, sens):\n",
    "    \"\"\"\n",
    "    x_img: (X,Y,Z) complex\n",
    "    sens : (C,X,Y,Z) complex\n",
    "    returns SoS magnitude volume (X,Y,Z)\n",
    "    \"\"\"\n",
    "    coil_imgs = x_img.unsqueeze(0) * sens  # (C,X,Y,Z)\n",
    "    sos = torch.sqrt((coil_imgs.abs()**2).sum(dim=0) + 1e-12)\n",
    "    return sos\n",
    "\n",
    "@torch.no_grad()\n",
    "def kspace_metrics(x, ksp, mask_omega, A, sens):\n",
    "    \"\"\"\n",
    "    x: (X,Y,Z) complex, ksp: (C,X,Y,Z) complex, mask_omega: (1,1,Y,Z) float/bool\n",
    "    Ω_MSE over acquired lines, Ωᶜ_E mean energy on unacquired.\n",
    "    \"\"\"\n",
    "    ones = torch.ones_like(mask_omega)\n",
    "    y_hat = A(x, ones, sens)          # (C,X,Y,Z)\n",
    "    diff  = y_hat - ksp\n",
    "    m_acq = mask_omega.bool()\n",
    "    m_un  = ~m_acq\n",
    "    # broadcasting across (C,X)\n",
    "    mse_acq   = ((diff.abs()**2)[..., m_acq.squeeze(0).squeeze(0)].mean()).item()\n",
    "    energy_un = ((y_hat.abs()**2)[..., m_un.squeeze(0).squeeze(0)].mean()).item()\n",
    "    return mse_acq, energy_un\n",
    "\n",
    "@torch.no_grad()\n",
    "def unroll_once(x, x_th, Ah_y, mask_omega, sens, lam, gamma):\n",
    "    r     = apply_prior_25d(prior, x, k=k_neighbors)  # residual in image space\n",
    "    x_til = x_th + gamma * r\n",
    "    rhs   = Ah_y + lam * x_til\n",
    "    x_new = solve_cg(rhs, mask_omega, sens, lam, cg_iters, cg_tol, x0=x)\n",
    "    return x_new\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_one_phase(ksp_p, sens, mask_omega, lam, gammas):\n",
    "    \"\"\"\n",
    "    Returns dict with chosen recon, cg_sos, recon_sos, chosen gamma and metrics per gamma.\n",
    "    \"\"\"\n",
    "    # CG-SENSE anchor on Ω (acquired lines)\n",
    "    y_acq = ksp_p * mask_omega\n",
    "    Ah_y  = AH(y_acq, mask_omega, sens)\n",
    "    x_th  = solve_cg(Ah_y, mask_omega, sens, lam, cg_iters, cg_tol, x0=None)\n",
    "\n",
    "    results = []\n",
    "    for g in gammas:\n",
    "        x = x_th.clone()\n",
    "        for _ in range(unroll):\n",
    "            x = unroll_once(x, x_th, Ah_y, mask_omega, sens, lam, g)\n",
    "        mse_acq, energy_un = kspace_metrics(x, ksp_p, mask_omega, A, sens)\n",
    "        results.append(dict(gamma=g, x=x, mse=mse_acq, e_un=energy_un))\n",
    "\n",
    "    # pick best under cap; else fallback to score = mse + eta*e_un\n",
    "    under = [r for r in results if r[\"e_un\"] <= energy_cap]\n",
    "    if len(under) > 0:\n",
    "        best = min(under, key=lambda r: r[\"mse\"])\n",
    "    else:\n",
    "        best = min(results, key=lambda r: r[\"mse\"] + fallback_eta * r[\"e_un\"])\n",
    "\n",
    "    # SoS volumes\n",
    "    cg_sos    = sos_from_image_and_sens(x_th, sens)\n",
    "    recon_sos = sos_from_image_and_sens(best[\"x\"], sens)\n",
    "\n",
    "    return dict(best=best, cg_sos=cg_sos, recon_sos=recon_sos, all=results)\n",
    "\n",
    "# ==== load data ====\n",
    "data = np.load(npz_path, allow_pickle=True)\n",
    "ksp_np   = data[\"ksp\"]    # expect (P,C,X,Y,Z) complex64\n",
    "sens_np  = data[\"sens\"]   # (C,X,Y,Z) complex64\n",
    "omega_np = data[\"omega\"]  # (P,Y,Z) float/bool\n",
    "\n",
    "# adapt shapes if ksp is (P,X,Y,Z,C)\n",
    "if ksp_np.shape[1] != sens_np.shape[0]:\n",
    "    # move coils axis to dim=1\n",
    "    # from (P,X,Y,Z,C) -> (P,C,X,Y,Z)\n",
    "    ksp_np = np.moveaxis(ksp_np, -1, 1)\n",
    "\n",
    "P, C, X, Y, Z = ksp_np.shape\n",
    "print(f\"Loaded: P={P}, C={C}, X={X}, Y={Y}, Z={Z}\")\n",
    "\n",
    "# to torch\n",
    "ksp_t  = to_torch(ksp_np)        # (P,C,X,Y,Z) complex\n",
    "sens_t = to_torch(sens_np)       # (C,X,Y,Z) complex\n",
    "\n",
    "rows = []\n",
    "t0 = time.time()\n",
    "\n",
    "prior = Pseudo3DUNet2p5D(in_ch=2*k_neighbors, base=32, residual_scale=0.1).to(device)\n",
    "\n",
    "ckpt_path = \"best_prior_100_epoch.pt\"\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "prior.load_state_dict(ckpt[\"model_state\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    prior.eval()\n",
    "    for p in range(1):\n",
    "        ksp_p  = ksp_t[p]  # (C,X,Y,Z)\n",
    "        mask_w = torch.from_numpy(omega_np[p]).to(device=device, dtype=torch.float32)\n",
    "        mask_w = mask_w.unsqueeze(0).unsqueeze(0)  # (1,1,Y,Z)\n",
    "\n",
    "        out = infer_one_phase(ksp_p, sens_t, mask_w, lam_fixed, gamma_grid)\n",
    "\n",
    "        # save SoS volumes\n",
    "        cg_sos_np    = out[\"cg_sos\"].cpu().numpy()\n",
    "        recon_sos_np = out[\"recon_sos\"].cpu().numpy()\n",
    "        np.save(os.path.join(out_dir, f\"phase_{p+1:02d}_cg_sos.npy\"),    cg_sos_np)\n",
    "        np.save(os.path.join(out_dir, f\"phase_{p+1:02d}_recon_sos.npy\"), recon_sos_np)\n",
    "\n",
    "        b = out[\"best\"]\n",
    "        rows.append({\n",
    "            \"phase\":   p+1,\n",
    "            \"lambda\":  lam_fixed,\n",
    "            \"gamma\":   float(b[\"gamma\"]),\n",
    "            \"Omega_MSE\": float(b[\"mse\"]),\n",
    "            \"OmegaC_E\":  float(b[\"e_un\"]),\n",
    "        })\n",
    "        print(f\"phase {p+1:02d} | γ={b['gamma']:.2e}  Ω_MSE={b['mse']:.3e}  Ωc_E={b['e_un']:.3e}\")\n",
    "\n",
    "# write CSV\n",
    "csv_path = os.path.join(out_dir, \"per_phase_metrics.csv\")\n",
    "with open(csv_path, \"w\", newline=\"\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"phase\",\"lambda\",\"gamma\",\"Omega_MSE\",\"OmegaC_E\"])\n",
    "    w.writeheader()\n",
    "    w.writerows(rows)\n",
    "\n",
    "print(f\"\\nSaved SoS volumes + CSV to: {out_dir}\")\n",
    "print(f\"Elapsed: {time.time()-t0:.1f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "napari-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
